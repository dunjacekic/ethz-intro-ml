{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Precision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-786b9780e911>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdagrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrecision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Precision'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from keras import Model\n",
    "from keras import Sequential\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Concatenate, Input, BatchNormalization\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import Precision\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import itertools as it\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 227940 entries, 0 to 227939\n",
      "Data columns (total 37 columns):\n",
      "pid                 227940 non-null int64\n",
      "Time                227940 non-null int64\n",
      "Age                 227940 non-null float64\n",
      "EtCO2               9783 non-null float64\n",
      "PTT                 10299 non-null float64\n",
      "BUN                 20105 non-null float64\n",
      "Lactate             10756 non-null float64\n",
      "Temp                81115 non-null float64\n",
      "Hgb                 22295 non-null float64\n",
      "HCO3                12559 non-null float64\n",
      "BaseExcess          19887 non-null float64\n",
      "RRate               187785 non-null float64\n",
      "Fibrinogen          2493 non-null float64\n",
      "Phosphate           11590 non-null float64\n",
      "WBC                 19083 non-null float64\n",
      "Creatinine          17792 non-null float64\n",
      "PaCO2               21043 non-null float64\n",
      "AST                 5761 non-null float64\n",
      "FiO2                26602 non-null float64\n",
      "Platelets           18035 non-null float64\n",
      "SaO2                13014 non-null float64\n",
      "Glucose             47036 non-null float64\n",
      "ABPm                195889 non-null float64\n",
      "Magnesium           17523 non-null float64\n",
      "Potassium           28393 non-null float64\n",
      "ABPd                152418 non-null float64\n",
      "Calcium             17830 non-null float64\n",
      "Alkalinephos        5708 non-null float64\n",
      "SpO2                195192 non-null float64\n",
      "Bilirubin_direct    719 non-null float64\n",
      "Chloride            13917 non-null float64\n",
      "Hct                 27297 non-null float64\n",
      "Heartrate           200128 non-null float64\n",
      "Bilirubin_total     5326 non-null float64\n",
      "TroponinI           3776 non-null float64\n",
      "ABPs                191650 non-null float64\n",
      "pH                  25046 non-null float64\n",
      "dtypes: float64(35), int64(2)\n",
      "memory usage: 64.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 1. Import data\n",
    "train_data = pd.read_csv('train_features.csv')\n",
    "df = pd.DataFrame(train_data)\n",
    "\n",
    "train_labels = pd.read_csv('train_labels.csv') # Last 4 columns are real-valued, everything else is boolean\n",
    "df2 = pd.DataFrame(train_labels)\n",
    "\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.  Pre-processing\n",
    "# 2a. Handle missing data\n",
    "df = df.fillna(df.mean())\n",
    "\n",
    "# 2b. Concatenate rows from a single patient, do not duplicate age\n",
    "df = df.drop(\"Time\", axis=1)\n",
    "num_patients = int(df.shape[0]/12)\n",
    "num_feats = int(df.shape[1]*12 - 12 - 11) # Remove patient id, extra age entries\n",
    "\n",
    "df_np = df.to_numpy()\n",
    "df_np_new = np.zeros((num_patients, num_feats))\n",
    "\n",
    "for patient_idx in range(num_patients):\n",
    "    df_np_new[0,:] = np.hstack((df_np[patient_idx,1:], df_np[patient_idx+1,2:], df_np[patient_idx+2,2:], \n",
    "                                df_np[patient_idx+3,2::], df_np[patient_idx+4,2:], df_np[patient_idx+5,2:], \n",
    "                                df_np[patient_idx+6,2::], df_np[patient_idx+7,2:], df_np[patient_idx+8,2:], \n",
    "                                df_np[patient_idx+9,2::], df_np[patient_idx+10,2:], df_np[patient_idx+11,2:]))\n",
    "\n",
    "# 2c. Process training labels\n",
    "df2 = df2.drop(\"pid\", axis=1)\n",
    "df2_np = df2.to_numpy()\n",
    "\n",
    "# Separate labels into classification and regression tasks\n",
    "num_labels = df2.shape[1]\n",
    "num_class_labels = num_labels - 4\n",
    "num_regress_labels = 4\n",
    "\n",
    "class_labels = df2_np[:,0:num_labels-4]\n",
    "regress_labels = df2_np[:,num_labels-4:]\n",
    "\n",
    "x_train = df_np_new\n",
    "y_train_cls = class_labels\n",
    "y_train_reg = regress_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d. Normalize, balance the data with cost-sensitive loss\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "\n",
    "# # reduce size of data for now\n",
    "# x_train = x_train[:2000,:]\n",
    "# y_train_cls = y_train_cls[:2000,:]\n",
    "# y_train_reg = y_train_reg[:2000,:]\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class RocCallback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred_train = self.model.predict(self.x)\n",
    "        roc_train = roc_auc_score(self.y, y_pred_train)\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        print('\\rroc-auc_train: %s - roc-auc_val: %s' % (str(round(roc_train,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.  Model architecture\n",
    "def binary_classifier(lr, decay, alpha):\n",
    "\n",
    "    x = Input(shape=(num_feats,))\n",
    "    h1 = Dense(30, activation='relu')(x)\n",
    "    h1n = BatchNormalization()(h1)\n",
    "    h2 = Dense(30, activation='relu')(h1)\n",
    "    h2n = BatchNormalization()(h2)\n",
    "    h3 = Dense(10, activation='relu')(h2)\n",
    "    h3n = BatchNormalization()(h3)\n",
    "    y = Dense(1, activation='sigmoid')(h3n)\n",
    "    \n",
    "    # To-do: Insert outputs for regression outputs\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    ada_grad = Adagrad(lr=lr, epsilon=1e-08, decay=decay)\n",
    "    #adam_grad = Adam(lr=0.1, beta_1=0.9, beta_2=0.9)\n",
    "\n",
    "    model.compile(optimizer=ada_grad, loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# 3.  Model architecture\n",
    "def regressor(lr, decay):\n",
    "    x = Input(shape=(num_feats,))\n",
    "    h1 = Dense(10, activation='relu', kernel_initializer='he_uniform')(x)\n",
    "    h1n = BatchNormalization()(h1)\n",
    "    h2 = Dense(10, activation='relu', kernel_initializer='he_uniform')(h1n)\n",
    "    h2n = BatchNormalization()(h2)\n",
    "    h3 = Dense(10, activation='relu', kernel_initializer='he_uniform')(h2n)\n",
    "    h3n = BatchNormalization()(h3)\n",
    "    h4 = Dense(10, activation='relu', kernel_initializer='he_uniform')(h3n)\n",
    "    h4n = BatchNormalization()(h4)\n",
    "    y = Dense(num_regress_labels, activation='linear', kernel_initializer='he_normal')(h4n)\n",
    "\n",
    "    # To-do: Insert outputs for regression outputs\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    ada_grad = Adagrad(lr=lr, epsilon=1e-08, decay=decay)\n",
    "    #adam_grad = Adam(lr=0.1, beta_1=0.9, beta_2=0.9)\n",
    "\n",
    "    model.compile(optimizer=ada_grad, loss='mean_squared_error',\n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [compute_class_weight('balanced', np.unique(y_train_cls[:,i]), y_train_cls[:,i]) for i in range(num_class_labels)]\n",
    "\n",
    "for i in range(num_class_labels):\n",
    "   max_ind = class_weights[i].argmax()\n",
    "   class_weights[i][max_ind] *= 1.1\n",
    "\n",
    "class_weights = [dict(enumerate(weights)) for weights in class_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6833225411900137, 1: 2.0500883045525904}\n",
      "Train on 17095 samples, validate on 1900 samples\n",
      "Epoch 1/5\n",
      "17095/17095 [==============================] - 5s 276us/step - loss: 0.7278 - acc: 0.4152 - val_loss: 0.7018 - val_acc: 0.2679\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-de8e2ed4c43e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcls_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 shuffle=True) \n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-e5e9de2280da>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mroc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0my_pred_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "# Try to overfit the model to a subset of the data\n",
    "cls_idx = 0\n",
    "n_splits=10\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "folds = skf.split(x_train, y_train_cls[:,cls_idx])\n",
    "\n",
    "lr=5e-3\n",
    "decay=0.1\n",
    "alpha=1e2\n",
    "cls_model = binary_classifier(lr, decay, alpha)\n",
    "\n",
    "batch_size = 20\n",
    "epochs = 5\n",
    "\n",
    "print(class_weights[cls_index])\n",
    "for train_idx, test_idx in folds:\n",
    "    sample_x = x_train[train_idx,:]\n",
    "    sample_y = y_train_cls[train_idx,cls_idx]\n",
    "    \n",
    "    test_x = x_train[test_idx,:]\n",
    "    test_y = y_train_cls[test_idx,cls_idx]\n",
    "    \n",
    "    roc = RocCallback(training_data=(sample_x, sample_y),\n",
    "                      validation_data=(test_x, test_y))\n",
    "        \n",
    "    cls_model.fit(sample_x, sample_y,\n",
    "                validation_data=(test_x, test_y),\n",
    "                batch_size=batch_size, epochs=epochs,\n",
    "                class_weight=class_weights[cls_index],\n",
    "                callbacks=[roc],\n",
    "                shuffle=True) \n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters to CV over\n",
    "params = {}\n",
    "params['lr'] = np.logspace(-4,1,8)\n",
    "params['decay'] = np.logspace(-2,1,5)\n",
    "params['batch_size'] = np.linspace(500,3500,3)\n",
    "\n",
    "best_reg = {'err':99e6,'params':None, 'train_losses':[], 'val_losses':[]}\n",
    "best_cls = [{'roc':0,'params':None,'acc':0, 'train_losses':[], 'val_losses':[]} for i in range(num_class_labels)]\n",
    "\n",
    "epochs = 25\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, mode='min')\n",
    "\n",
    "# Initialize splitter\n",
    "n_splits=6\n",
    "skf = StratifiedKFold(n_splits=n_splits)\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Get parameter combinations\n",
    "keys = params.keys()\n",
    "values = (params[key] for key in keys)\n",
    "combinations = [dict(zip(keys, combination)) for combination in it.product(*values)]\n",
    "\n",
    "for param_config in combinations:\n",
    "    print(param_config)\n",
    "    lr = param_config['lr']\n",
    "    decay = param_config['decay']\n",
    "    batch_size = int(param_config['batch_size'])\n",
    "    \n",
    "    # Generate folds\n",
    "    cls_folds = [skf.split(x_train, y_train_cls[:,i]) for i in range(num_class_labels)]\n",
    "    reg_folds = kf.split(x_train, y_train_reg)\n",
    "    \n",
    "    # Classifiers\n",
    "    for i in range(1):\n",
    "        \n",
    "        rocs, accs, train_losses, val_losses = [], [], [], []\n",
    "        for train_idx, test_idx in cls_folds[i]:\n",
    "            model = binary_classifier(lr, decay)\n",
    "            x, y = x_train[train_idx], y_train_cls[train_idx,i]\n",
    "            x_val, y_val = x_train[test_idx] , y_train_cls[test_idx,i]\n",
    "            history = model.fit(x, y,\n",
    "                               batch_size=batch_size, epochs=epochs,\n",
    "                               validation_data=(x_val,y_val),\n",
    "                               class_weight=class_weights[i],\n",
    "                               callbacks=[es],\n",
    "                               shuffle=True) \n",
    "            \n",
    "            accs.append(history.history['acc'][-1])\n",
    "            rocs.append(history.history['auroc'][-1])\n",
    "            train_losses.append(history.history['loss'])\n",
    "            val_losses.append(history.history['val_loss'])\n",
    "\n",
    "            del model\n",
    "            \n",
    "        if mean(rocs) > best_cls[i]['roc']:\n",
    "            best_cls[i]['params'] = param_config\n",
    "            best_cls[i]['roc'], best_cls[i]['acc'] = mean(rocs), mean(accs)\n",
    "            best_cls[i]['train_losses'] = train_losses\n",
    "            best_cls[i]['val_losses'] = val_losses\n",
    "            print(f\"New best roc for class #{i} : {best_cls[i]['roc']}\")\n",
    "                  \n",
    "#     # Regressor\n",
    "#     errs = []\n",
    "#     for train_idx, test_idx in reg_folds:\n",
    "#         model = regressor(lr, decay)\n",
    "#         x, y = x_train[train_idx], y_train_reg[train_idx,:]\n",
    "#         x_val, y_val = x_train[test_idx] , y_train_reg[test_idx,:]\n",
    "#         history = model.fit(x, y,\n",
    "#                            batch_size=batch_size, epochs=epochs,\n",
    "#                            validation_data=(x_val,y_val)) \n",
    "\n",
    "#         errs.append(history.history['mean_squared_error'][-1])\n",
    "#         del model\n",
    "\n",
    "#     if mean(errs) < best_reg['err']:\n",
    "#         best_reg['params'] = param_config\n",
    "#         best_reg['err'] = mean(errs)\n",
    "\n",
    "#         print(f\"New best err for regressor : {best_reg['err']}\")\n",
    "              \n",
    "print('<<<FINAL BEST PARAMS>>>')\n",
    "for i in range(num_class_labels):\n",
    "    print(\"----------\")\n",
    "    print(f\"Class {i}\")\n",
    "    print(\"----------\")\n",
    "    print(best_cls[i]['params'])  \n",
    "    print(f\"ROC = {best_cls[i]['roc']}\")\n",
    "          \n",
    "print(\"----------\")\n",
    "print(\"Regressor\")\n",
    "print(\"----------\")\n",
    "print(best_reg['params'])\n",
    "print(f\"MSE = {best_reg['err']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit5446efe966914acba572d976b9a4daba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
