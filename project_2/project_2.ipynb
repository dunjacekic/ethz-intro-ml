{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro ML - Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_features.csv')\n",
    "labels = pd.read_csv('train_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time_features(data, n_samples):\n",
    "    x = []\n",
    "    features = [np.nanmedian, np.nanmean, np.nanvar, np.nanmin,\n",
    "           np.nanmax]\n",
    "    for index in range(int(data.shape[0] / n_samples)):\n",
    "        patient_data = data[n_samples * index:n_samples * (index + 1), 2:]\n",
    "        feature_values = np.empty((len(features), train_data_numpy[:, 2:].shape[1]))\n",
    "        for i, feature in enumerate(features):\n",
    "            feature_values[i] = feature(patient_data, axis=0)\n",
    "        x.append(feature_values.ravel())\n",
    "    ids = data[0::n_samples, 0].astype(np.uint16).copy()\n",
    "    bits = np.unpackbits(\n",
    "        np.expand_dims(ids, axis=1).view(np.uint8),\n",
    "            axis=1)\n",
    "    x = np.hstack((bits, np.array(x)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yardenas/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: Mean of empty slice\n",
      "  if __name__ == '__main__':\n",
      "/home/yardenas/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  if __name__ == '__main__':\n",
      "/home/yardenas/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: All-NaN slice encountered\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "train_data_numpy = train_data.to_numpy()\n",
    "x_train = calculate_time_features(train_data_numpy, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Pipeline - Subtask 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ids = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST',\n",
    "         'LABEL_Alkalinephos', 'LABEL_Bilirubin_total', \n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "y_train = labels[labels_ids].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64274840\n",
      "Iteration 2, loss = 0.56896656\n",
      "Iteration 3, loss = 0.53228856\n",
      "Iteration 4, loss = 0.50133224\n",
      "Iteration 5, loss = 0.47465596\n",
      "Iteration 6, loss = 0.45177469\n",
      "Iteration 7, loss = 0.43369398\n",
      "Iteration 8, loss = 0.41945798\n",
      "Iteration 9, loss = 0.40833100\n",
      "Iteration 10, loss = 0.39925023\n",
      "Iteration 11, loss = 0.39153959\n",
      "Iteration 12, loss = 0.38513620\n",
      "Iteration 13, loss = 0.37940331\n",
      "Iteration 14, loss = 0.37447019\n",
      "Iteration 15, loss = 0.36982887\n",
      "Iteration 16, loss = 0.36538219\n",
      "Iteration 17, loss = 0.36148769\n",
      "Iteration 18, loss = 0.35798073\n",
      "Iteration 19, loss = 0.35429249\n",
      "Iteration 20, loss = 0.35097011\n",
      "Iteration 21, loss = 0.34748475\n",
      "Iteration 22, loss = 0.34431427\n",
      "Iteration 23, loss = 0.34117717\n",
      "Iteration 24, loss = 0.33806943\n",
      "Iteration 25, loss = 0.33515382\n",
      "Iteration 26, loss = 0.33229305\n",
      "Iteration 27, loss = 0.32940784\n",
      "Iteration 28, loss = 0.32651491\n",
      "Iteration 29, loss = 0.32392486\n",
      "Iteration 30, loss = 0.32127641\n",
      "Iteration 31, loss = 0.31854807\n",
      "Iteration 32, loss = 0.31600556\n",
      "Iteration 33, loss = 0.31392088\n",
      "Iteration 34, loss = 0.31067087\n",
      "Iteration 35, loss = 0.30860251\n",
      "Iteration 36, loss = 0.30603491\n",
      "Iteration 37, loss = 0.30382503\n",
      "Iteration 38, loss = 0.30122207\n",
      "Iteration 39, loss = 0.29843843\n",
      "Iteration 40, loss = 0.29619401\n",
      "Iteration 41, loss = 0.29364456\n",
      "Iteration 42, loss = 0.29197438\n",
      "Iteration 43, loss = 0.28925705\n",
      "Iteration 44, loss = 0.28702790\n",
      "Iteration 45, loss = 0.28489687\n",
      "Iteration 46, loss = 0.28228629\n",
      "Iteration 47, loss = 0.27989469\n",
      "Iteration 48, loss = 0.27797489\n",
      "Iteration 49, loss = 0.27549277\n",
      "Iteration 50, loss = 0.27335495\n",
      "Iteration 51, loss = 0.27097396\n",
      "Iteration 52, loss = 0.26890448\n",
      "Iteration 53, loss = 0.26649936\n",
      "Iteration 54, loss = 0.26441169\n",
      "Iteration 55, loss = 0.26228600\n",
      "Iteration 56, loss = 0.25996939\n",
      "Iteration 57, loss = 0.25764396\n",
      "Iteration 58, loss = 0.25578205\n",
      "Iteration 59, loss = 0.25321551\n",
      "Iteration 60, loss = 0.25142523\n",
      "Iteration 61, loss = 0.24925209\n",
      "Iteration 62, loss = 0.24709512\n",
      "Iteration 63, loss = 0.24509342\n",
      "Iteration 64, loss = 0.24301555\n",
      "Iteration 65, loss = 0.24105736\n",
      "Iteration 66, loss = 0.23913163\n",
      "Iteration 67, loss = 0.23657191\n",
      "Iteration 68, loss = 0.23462525\n",
      "Iteration 69, loss = 0.23273777\n",
      "Iteration 70, loss = 0.23101231\n",
      "Iteration 71, loss = 0.22880271\n",
      "Iteration 72, loss = 0.22670328\n",
      "Iteration 73, loss = 0.22476297\n",
      "Iteration 74, loss = 0.22252796\n",
      "Iteration 75, loss = 0.22057953\n",
      "Iteration 76, loss = 0.21834549\n",
      "Iteration 77, loss = 0.21702493\n",
      "Iteration 78, loss = 0.21464476\n",
      "Iteration 79, loss = 0.21309368\n",
      "Iteration 80, loss = 0.21071662\n",
      "Iteration 81, loss = 0.20879806\n",
      "Iteration 82, loss = 0.20693329\n",
      "Iteration 83, loss = 0.20526462\n",
      "Iteration 84, loss = 0.20381727\n",
      "Iteration 85, loss = 0.20135360\n",
      "Iteration 86, loss = 0.19931507\n",
      "Iteration 87, loss = 0.19744024\n",
      "Iteration 88, loss = 0.19556809\n",
      "Iteration 89, loss = 0.19370809\n",
      "Iteration 90, loss = 0.19221038\n",
      "Iteration 91, loss = 0.19029825\n",
      "Iteration 92, loss = 0.18847119\n",
      "Iteration 93, loss = 0.18700771\n",
      "Iteration 94, loss = 0.18492788\n",
      "Iteration 95, loss = 0.18328881\n",
      "Iteration 96, loss = 0.18132293\n",
      "Iteration 97, loss = 0.17954915\n",
      "Iteration 98, loss = 0.17788584\n",
      "Iteration 99, loss = 0.17597232\n",
      "Iteration 100, loss = 0.17402547\n",
      "Iteration 101, loss = 0.17243369\n",
      "Iteration 102, loss = 0.17072607\n",
      "Iteration 103, loss = 0.16928073\n",
      "Iteration 104, loss = 0.16764521\n",
      "Iteration 105, loss = 0.16573848\n",
      "Iteration 106, loss = 0.16424191\n",
      "Iteration 107, loss = 0.16214629\n",
      "Iteration 108, loss = 0.16043253\n",
      "Iteration 109, loss = 0.15873106\n",
      "Iteration 110, loss = 0.15665537\n",
      "Iteration 111, loss = 0.15545579\n",
      "Iteration 112, loss = 0.15354671\n",
      "Iteration 113, loss = 0.15218448\n",
      "Iteration 114, loss = 0.15088338\n",
      "Iteration 115, loss = 0.14886234\n",
      "Iteration 116, loss = 0.14689616\n",
      "Iteration 117, loss = 0.14520170\n",
      "Iteration 118, loss = 0.14372823\n",
      "Iteration 119, loss = 0.14225985\n",
      "Iteration 120, loss = 0.14109764\n",
      "Iteration 121, loss = 0.13916312\n",
      "Iteration 122, loss = 0.13785064\n",
      "Iteration 123, loss = 0.13617453\n",
      "Iteration 124, loss = 0.13438660\n",
      "Iteration 125, loss = 0.13283147\n",
      "Iteration 126, loss = 0.13108093\n",
      "Iteration 127, loss = 0.12977066\n",
      "Iteration 128, loss = 0.12849609\n",
      "Iteration 129, loss = 0.12658428\n",
      "Iteration 130, loss = 0.12496785\n",
      "Iteration 131, loss = 0.12365560\n",
      "Iteration 132, loss = 0.12229342\n",
      "Iteration 133, loss = 0.12064218\n",
      "Iteration 134, loss = 0.11916373\n",
      "Iteration 135, loss = 0.11770112\n",
      "Iteration 136, loss = 0.11629803\n",
      "Iteration 137, loss = 0.11469078\n",
      "Iteration 138, loss = 0.11339115\n",
      "Iteration 139, loss = 0.11140854\n",
      "Iteration 140, loss = 0.11056005\n",
      "Iteration 141, loss = 0.10853860\n",
      "Iteration 142, loss = 0.10759187\n",
      "Iteration 143, loss = 0.10592809\n",
      "Iteration 144, loss = 0.10501701\n",
      "Iteration 145, loss = 0.10343361\n",
      "Iteration 146, loss = 0.10186530\n",
      "Iteration 147, loss = 0.10080079\n",
      "Iteration 148, loss = 0.09916197\n",
      "Iteration 149, loss = 0.09787516\n",
      "Iteration 150, loss = 0.09712152\n",
      "Iteration 151, loss = 0.09547144\n",
      "Iteration 152, loss = 0.09392460\n",
      "Iteration 153, loss = 0.09277454\n",
      "Iteration 154, loss = 0.09131144\n",
      "Iteration 155, loss = 0.09089968\n",
      "Iteration 156, loss = 0.08861070\n",
      "Iteration 157, loss = 0.08757978\n",
      "Iteration 158, loss = 0.08629435\n",
      "Iteration 159, loss = 0.08512684\n",
      "Iteration 160, loss = 0.08390703\n",
      "Iteration 161, loss = 0.08265669\n",
      "Iteration 162, loss = 0.08113788\n",
      "Iteration 163, loss = 0.07982407\n",
      "Iteration 164, loss = 0.07882639\n",
      "Iteration 165, loss = 0.07783140\n",
      "Iteration 166, loss = 0.07612205\n",
      "Iteration 167, loss = 0.07592846\n",
      "Iteration 168, loss = 0.07434314\n",
      "Iteration 169, loss = 0.07275735\n",
      "Iteration 170, loss = 0.07200210\n",
      "Iteration 171, loss = 0.07080943\n",
      "Iteration 172, loss = 0.06917575\n",
      "Iteration 173, loss = 0.06811871\n",
      "Iteration 174, loss = 0.06698089\n",
      "Iteration 175, loss = 0.06585311\n",
      "Iteration 176, loss = 0.06462080\n",
      "Iteration 177, loss = 0.06357666\n",
      "Iteration 178, loss = 0.06235657\n",
      "Iteration 179, loss = 0.06128837\n",
      "Iteration 180, loss = 0.06014368\n",
      "Iteration 181, loss = 0.05914088\n",
      "Iteration 182, loss = 0.05817409\n",
      "Iteration 183, loss = 0.05704473\n",
      "Iteration 184, loss = 0.05570718\n",
      "Iteration 185, loss = 0.05477314\n",
      "Iteration 186, loss = 0.05361911\n",
      "Iteration 187, loss = 0.05300237\n",
      "Iteration 188, loss = 0.05221831\n",
      "Iteration 189, loss = 0.05087740\n",
      "Iteration 190, loss = 0.04970404\n",
      "Iteration 191, loss = 0.04860836\n",
      "Iteration 192, loss = 0.04847017\n",
      "Iteration 193, loss = 0.04665404\n",
      "Iteration 194, loss = 0.04588453\n",
      "Iteration 195, loss = 0.04499902\n",
      "Iteration 196, loss = 0.04391728\n",
      "Iteration 197, loss = 0.04287566\n",
      "Iteration 198, loss = 0.04247702\n",
      "Iteration 199, loss = 0.04122688\n",
      "Iteration 200, loss = 0.04020097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yardenas/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71589515\n",
      "Iteration 2, loss = 0.48908256\n",
      "Iteration 3, loss = 0.37960369\n",
      "Iteration 4, loss = 0.32328801\n",
      "Iteration 5, loss = 0.29390824\n",
      "Iteration 6, loss = 0.27599881\n",
      "Iteration 7, loss = 0.26335490\n",
      "Iteration 8, loss = 0.25401969\n",
      "Iteration 9, loss = 0.24696556\n",
      "Iteration 10, loss = 0.24130611\n",
      "Iteration 11, loss = 0.23656197\n",
      "Iteration 12, loss = 0.23220718\n",
      "Iteration 13, loss = 0.22847423\n",
      "Iteration 14, loss = 0.22501969\n",
      "Iteration 15, loss = 0.22184697\n",
      "Iteration 16, loss = 0.21878799\n",
      "Iteration 17, loss = 0.21620716\n",
      "Iteration 18, loss = 0.21363840\n",
      "Iteration 19, loss = 0.21128959\n",
      "Iteration 20, loss = 0.20903222\n",
      "Iteration 21, loss = 0.20686844\n",
      "Iteration 22, loss = 0.20493945\n",
      "Iteration 23, loss = 0.20284430\n",
      "Iteration 24, loss = 0.20092991\n",
      "Iteration 25, loss = 0.19907204\n",
      "Iteration 26, loss = 0.19726760\n",
      "Iteration 27, loss = 0.19556585\n",
      "Iteration 28, loss = 0.19369517\n",
      "Iteration 29, loss = 0.19200637\n",
      "Iteration 30, loss = 0.19028558\n",
      "Iteration 31, loss = 0.18885854\n",
      "Iteration 32, loss = 0.18719289\n",
      "Iteration 33, loss = 0.18547055\n",
      "Iteration 34, loss = 0.18399700\n",
      "Iteration 35, loss = 0.18253914\n",
      "Iteration 36, loss = 0.18114121\n",
      "Iteration 37, loss = 0.17960111\n",
      "Iteration 38, loss = 0.17798067\n",
      "Iteration 39, loss = 0.17644861\n",
      "Iteration 40, loss = 0.17497988\n",
      "Iteration 41, loss = 0.17355424\n",
      "Iteration 42, loss = 0.17203048\n",
      "Iteration 43, loss = 0.17047906\n",
      "Iteration 44, loss = 0.16908157\n",
      "Iteration 45, loss = 0.16758940\n",
      "Iteration 46, loss = 0.16605056\n",
      "Iteration 47, loss = 0.16450637\n",
      "Iteration 48, loss = 0.16299030\n",
      "Iteration 49, loss = 0.16157683\n",
      "Iteration 50, loss = 0.16018424\n",
      "Iteration 51, loss = 0.15873970\n",
      "Iteration 52, loss = 0.15729206\n",
      "Iteration 53, loss = 0.15556982\n",
      "Iteration 54, loss = 0.15413056\n",
      "Iteration 55, loss = 0.15253815\n",
      "Iteration 56, loss = 0.15087906\n",
      "Iteration 57, loss = 0.14941023\n",
      "Iteration 58, loss = 0.14795563\n",
      "Iteration 59, loss = 0.14628560\n",
      "Iteration 60, loss = 0.14466591\n",
      "Iteration 61, loss = 0.14323230\n",
      "Iteration 62, loss = 0.14146440\n",
      "Iteration 63, loss = 0.13993631\n",
      "Iteration 64, loss = 0.13829231\n",
      "Iteration 65, loss = 0.13673195\n",
      "Iteration 66, loss = 0.13509803\n",
      "Iteration 67, loss = 0.13341631\n",
      "Iteration 68, loss = 0.13156621\n",
      "Iteration 69, loss = 0.12996265\n",
      "Iteration 70, loss = 0.12828169\n",
      "Iteration 71, loss = 0.12658921\n",
      "Iteration 72, loss = 0.12496428\n",
      "Iteration 73, loss = 0.12318266\n",
      "Iteration 74, loss = 0.12153720\n",
      "Iteration 75, loss = 0.11975680\n",
      "Iteration 76, loss = 0.11812783\n",
      "Iteration 77, loss = 0.11655160\n",
      "Iteration 78, loss = 0.11479365\n",
      "Iteration 79, loss = 0.11275470\n",
      "Iteration 80, loss = 0.11124082\n",
      "Iteration 81, loss = 0.10951180\n",
      "Iteration 82, loss = 0.10797491\n",
      "Iteration 83, loss = 0.10611623\n",
      "Iteration 84, loss = 0.10454307\n",
      "Iteration 85, loss = 0.10275166\n",
      "Iteration 86, loss = 0.10133801\n",
      "Iteration 87, loss = 0.09949844\n",
      "Iteration 88, loss = 0.09805407\n",
      "Iteration 89, loss = 0.09602948\n",
      "Iteration 90, loss = 0.09453151\n",
      "Iteration 91, loss = 0.09316098\n",
      "Iteration 92, loss = 0.09143931\n",
      "Iteration 93, loss = 0.08982618\n",
      "Iteration 94, loss = 0.08842490\n",
      "Iteration 95, loss = 0.08673664\n",
      "Iteration 96, loss = 0.08533171\n",
      "Iteration 97, loss = 0.08356051\n",
      "Iteration 98, loss = 0.08229885\n",
      "Iteration 99, loss = 0.08077853\n",
      "Iteration 100, loss = 0.07928820\n",
      "Iteration 101, loss = 0.07785928\n",
      "Iteration 102, loss = 0.07635935\n",
      "Iteration 103, loss = 0.07483927\n",
      "Iteration 104, loss = 0.07318316\n",
      "Iteration 105, loss = 0.07216265\n",
      "Iteration 106, loss = 0.07064922\n",
      "Iteration 107, loss = 0.06911525\n",
      "Iteration 108, loss = 0.06768643\n",
      "Iteration 109, loss = 0.06633659\n",
      "Iteration 110, loss = 0.06498151\n",
      "Iteration 111, loss = 0.06380135\n",
      "Iteration 112, loss = 0.06244132\n",
      "Iteration 113, loss = 0.06078236\n",
      "Iteration 114, loss = 0.05946796\n",
      "Iteration 115, loss = 0.05817428\n",
      "Iteration 116, loss = 0.05712762\n",
      "Iteration 117, loss = 0.05573884\n",
      "Iteration 118, loss = 0.05434814\n",
      "Iteration 119, loss = 0.05319883\n",
      "Iteration 120, loss = 0.05167296\n",
      "Iteration 121, loss = 0.05071102\n",
      "Iteration 122, loss = 0.04949296\n",
      "Iteration 123, loss = 0.04816847\n",
      "Iteration 124, loss = 0.04689145\n",
      "Iteration 125, loss = 0.04592035\n",
      "Iteration 126, loss = 0.04494214\n",
      "Iteration 127, loss = 0.04374267\n",
      "Iteration 128, loss = 0.04251723\n",
      "Iteration 129, loss = 0.04130303\n",
      "Iteration 130, loss = 0.04014832\n",
      "Iteration 131, loss = 0.03920638\n",
      "Iteration 132, loss = 0.03821980\n",
      "Iteration 133, loss = 0.03699313\n",
      "Iteration 134, loss = 0.03608062\n",
      "Iteration 135, loss = 0.03502602\n",
      "Iteration 136, loss = 0.03392238\n",
      "Iteration 137, loss = 0.03303043\n",
      "Iteration 138, loss = 0.03222755\n",
      "Iteration 139, loss = 0.03145805\n",
      "Iteration 140, loss = 0.03020814\n",
      "Iteration 141, loss = 0.02958079\n",
      "Iteration 142, loss = 0.02869582\n",
      "Iteration 143, loss = 0.02770134\n",
      "Iteration 144, loss = 0.02699379\n",
      "Iteration 145, loss = 0.02615945\n",
      "Iteration 146, loss = 0.02539486\n",
      "Iteration 147, loss = 0.02446187\n",
      "Iteration 148, loss = 0.02388785\n",
      "Iteration 149, loss = 0.02316726\n",
      "Iteration 150, loss = 0.02236630\n",
      "Iteration 151, loss = 0.02164184\n",
      "Iteration 152, loss = 0.02106876\n",
      "Iteration 153, loss = 0.02031690\n",
      "Iteration 154, loss = 0.01978573\n",
      "Iteration 155, loss = 0.01916801\n",
      "Iteration 156, loss = 0.01854821\n",
      "Iteration 157, loss = 0.01801273\n",
      "Iteration 158, loss = 0.01735948\n",
      "Iteration 159, loss = 0.01671489\n",
      "Iteration 160, loss = 0.01624854\n",
      "Iteration 161, loss = 0.01567143\n",
      "Iteration 162, loss = 0.01516032\n",
      "Iteration 163, loss = 0.01465470\n",
      "Iteration 164, loss = 0.01418253\n",
      "Iteration 165, loss = 0.01363184\n",
      "Iteration 166, loss = 0.01328045\n",
      "Iteration 167, loss = 0.01276841\n",
      "Iteration 168, loss = 0.01233413\n",
      "Iteration 169, loss = 0.01185788\n",
      "Iteration 170, loss = 0.01151023\n",
      "Iteration 171, loss = 0.01099883\n",
      "Iteration 172, loss = 0.01071824\n",
      "Iteration 173, loss = 0.01023348\n",
      "Iteration 174, loss = 0.00979973\n",
      "Iteration 175, loss = 0.00949491\n",
      "Iteration 176, loss = 0.00906636\n",
      "Iteration 177, loss = 0.00872249\n",
      "Iteration 178, loss = 0.00847005\n",
      "Iteration 179, loss = 0.00810245\n",
      "Iteration 180, loss = 0.00776057\n",
      "Iteration 181, loss = 0.00743571\n",
      "Iteration 182, loss = 0.00709311\n",
      "Iteration 183, loss = 0.00687019\n",
      "Iteration 184, loss = 0.00657740\n",
      "Iteration 185, loss = 0.00629847\n",
      "Iteration 186, loss = 0.00600995\n",
      "Iteration 187, loss = 0.00577342\n",
      "Iteration 188, loss = 0.00553125\n",
      "Iteration 189, loss = 0.00531815\n",
      "Iteration 190, loss = 0.00514065\n",
      "Iteration 191, loss = 0.00495395\n",
      "Iteration 192, loss = 0.00467744\n",
      "Iteration 193, loss = 0.00448244\n",
      "Iteration 194, loss = 0.00428632\n",
      "Iteration 195, loss = 0.00414060\n",
      "Iteration 196, loss = 0.00402934\n",
      "Iteration 197, loss = 0.00382366\n",
      "Iteration 198, loss = 0.00366665\n",
      "Iteration 199, loss = 0.00352837\n",
      "Iteration 200, loss = 0.00341577\n",
      "Iteration 1, loss = 0.66619453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yardenas/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.59765059\n",
      "Iteration 3, loss = 0.57009081\n",
      "Iteration 4, loss = 0.55106382\n",
      "Iteration 5, loss = 0.53693808\n",
      "Iteration 6, loss = 0.52634604\n",
      "Iteration 7, loss = 0.51813915\n",
      "Iteration 8, loss = 0.51192529\n",
      "Iteration 9, loss = 0.50673144\n",
      "Iteration 10, loss = 0.50220034\n",
      "Iteration 11, loss = 0.49820462\n",
      "Iteration 12, loss = 0.49479674\n",
      "Iteration 13, loss = 0.49165333\n",
      "Iteration 14, loss = 0.48824877\n",
      "Iteration 15, loss = 0.48535864\n",
      "Iteration 16, loss = 0.48266347\n",
      "Iteration 17, loss = 0.47995874\n",
      "Iteration 18, loss = 0.47745254\n",
      "Iteration 19, loss = 0.47537911\n",
      "Iteration 20, loss = 0.47298840\n",
      "Iteration 21, loss = 0.47050557\n",
      "Iteration 22, loss = 0.46822766\n",
      "Iteration 23, loss = 0.46619347\n",
      "Iteration 24, loss = 0.46415754\n",
      "Iteration 25, loss = 0.46209110\n",
      "Iteration 26, loss = 0.45993991\n",
      "Iteration 27, loss = 0.45803707\n",
      "Iteration 28, loss = 0.45603799\n",
      "Iteration 29, loss = 0.45377515\n",
      "Iteration 30, loss = 0.45178459\n",
      "Iteration 31, loss = 0.44975915\n",
      "Iteration 32, loss = 0.44781409\n",
      "Iteration 33, loss = 0.44574627\n",
      "Iteration 34, loss = 0.44393431\n",
      "Iteration 35, loss = 0.44169748\n",
      "Iteration 36, loss = 0.44008515\n",
      "Iteration 37, loss = 0.43775172\n",
      "Iteration 38, loss = 0.43564619\n",
      "Iteration 39, loss = 0.43362311\n",
      "Iteration 40, loss = 0.43148819\n",
      "Iteration 41, loss = 0.42957495\n",
      "Iteration 42, loss = 0.42749144\n",
      "Iteration 43, loss = 0.42524843\n",
      "Iteration 44, loss = 0.42329256\n",
      "Iteration 45, loss = 0.42150442\n",
      "Iteration 46, loss = 0.41933248\n",
      "Iteration 47, loss = 0.41729114\n",
      "Iteration 48, loss = 0.41534483\n",
      "Iteration 49, loss = 0.41289913\n",
      "Iteration 50, loss = 0.41068357\n",
      "Iteration 51, loss = 0.40920302\n",
      "Iteration 52, loss = 0.40681019\n",
      "Iteration 53, loss = 0.40438491\n",
      "Iteration 54, loss = 0.40255733\n",
      "Iteration 55, loss = 0.40013621\n",
      "Iteration 56, loss = 0.39821654\n",
      "Iteration 57, loss = 0.39603915\n",
      "Iteration 58, loss = 0.39382522\n",
      "Iteration 59, loss = 0.39165238\n",
      "Iteration 60, loss = 0.38984757\n",
      "Iteration 61, loss = 0.38740376\n",
      "Iteration 62, loss = 0.38559817\n",
      "Iteration 63, loss = 0.38315073\n",
      "Iteration 64, loss = 0.38097979\n",
      "Iteration 65, loss = 0.37897753\n",
      "Iteration 66, loss = 0.37706325\n",
      "Iteration 67, loss = 0.37472868\n",
      "Iteration 68, loss = 0.37284405\n",
      "Iteration 69, loss = 0.37084389\n",
      "Iteration 70, loss = 0.36857823\n",
      "Iteration 71, loss = 0.36681152\n",
      "Iteration 72, loss = 0.36407207\n",
      "Iteration 73, loss = 0.36190349\n",
      "Iteration 74, loss = 0.36019809\n",
      "Iteration 75, loss = 0.35828803\n",
      "Iteration 76, loss = 0.35576070\n",
      "Iteration 77, loss = 0.35377507\n",
      "Iteration 78, loss = 0.35168028\n",
      "Iteration 79, loss = 0.34926070\n",
      "Iteration 80, loss = 0.34686999\n",
      "Iteration 81, loss = 0.34507929\n",
      "Iteration 82, loss = 0.34306063\n",
      "Iteration 83, loss = 0.34113100\n",
      "Iteration 84, loss = 0.33864267\n",
      "Iteration 85, loss = 0.33629780\n",
      "Iteration 86, loss = 0.33409596\n",
      "Iteration 87, loss = 0.33210416\n",
      "Iteration 88, loss = 0.33019428\n",
      "Iteration 89, loss = 0.32822508\n",
      "Iteration 90, loss = 0.32579163\n",
      "Iteration 91, loss = 0.32381694\n",
      "Iteration 92, loss = 0.32167879\n",
      "Iteration 93, loss = 0.32017946\n",
      "Iteration 94, loss = 0.31777201\n",
      "Iteration 95, loss = 0.31529903\n",
      "Iteration 96, loss = 0.31323280\n",
      "Iteration 97, loss = 0.31108951\n",
      "Iteration 98, loss = 0.30933054\n",
      "Iteration 99, loss = 0.30722216\n",
      "Iteration 100, loss = 0.30526622\n",
      "Iteration 101, loss = 0.30346270\n",
      "Iteration 102, loss = 0.30106164\n",
      "Iteration 103, loss = 0.29868254\n",
      "Iteration 104, loss = 0.29733893\n",
      "Iteration 105, loss = 0.29475585\n",
      "Iteration 106, loss = 0.29261608\n",
      "Iteration 107, loss = 0.29082407\n",
      "Iteration 108, loss = 0.28844316\n",
      "Iteration 109, loss = 0.28669643\n",
      "Iteration 110, loss = 0.28399614\n",
      "Iteration 111, loss = 0.28288190\n",
      "Iteration 112, loss = 0.28048728\n",
      "Iteration 113, loss = 0.27821128\n",
      "Iteration 114, loss = 0.27712284\n",
      "Iteration 115, loss = 0.27458383\n",
      "Iteration 116, loss = 0.27284016\n",
      "Iteration 117, loss = 0.27056624\n",
      "Iteration 118, loss = 0.26841988\n",
      "Iteration 119, loss = 0.26656449\n",
      "Iteration 120, loss = 0.26409176\n",
      "Iteration 121, loss = 0.26281105\n",
      "Iteration 122, loss = 0.26047447\n",
      "Iteration 123, loss = 0.25846899\n",
      "Iteration 124, loss = 0.25707160\n",
      "Iteration 125, loss = 0.25516937\n",
      "Iteration 126, loss = 0.25319352\n",
      "Iteration 127, loss = 0.25070818\n",
      "Iteration 128, loss = 0.24878428\n",
      "Iteration 129, loss = 0.24717370\n",
      "Iteration 130, loss = 0.24536443\n",
      "Iteration 131, loss = 0.24351755\n",
      "Iteration 132, loss = 0.24127634\n",
      "Iteration 133, loss = 0.24007758\n",
      "Iteration 134, loss = 0.23789060\n",
      "Iteration 135, loss = 0.23571953\n",
      "Iteration 136, loss = 0.23377594\n",
      "Iteration 137, loss = 0.23253604\n",
      "Iteration 138, loss = 0.23075445\n",
      "Iteration 139, loss = 0.22857109\n",
      "Iteration 140, loss = 0.22719220\n",
      "Iteration 141, loss = 0.22483719\n",
      "Iteration 142, loss = 0.22338774\n",
      "Iteration 143, loss = 0.22166040\n",
      "Iteration 144, loss = 0.22035902\n",
      "Iteration 145, loss = 0.21819164\n",
      "Iteration 146, loss = 0.21662377\n",
      "Iteration 147, loss = 0.21492646\n",
      "Iteration 148, loss = 0.21311364\n",
      "Iteration 149, loss = 0.21105727\n",
      "Iteration 150, loss = 0.20968435\n",
      "Iteration 151, loss = 0.20822676\n",
      "Iteration 152, loss = 0.20622236\n",
      "Iteration 153, loss = 0.20443876\n",
      "Iteration 154, loss = 0.20349540\n",
      "Iteration 155, loss = 0.20147838\n",
      "Iteration 156, loss = 0.19959158\n",
      "Iteration 157, loss = 0.19787408\n",
      "Iteration 158, loss = 0.19646710\n",
      "Iteration 159, loss = 0.19503863\n",
      "Iteration 160, loss = 0.19346715\n",
      "Iteration 161, loss = 0.19158715\n",
      "Iteration 162, loss = 0.18964758\n",
      "Iteration 163, loss = 0.18883915\n",
      "Iteration 164, loss = 0.18685887\n",
      "Iteration 165, loss = 0.18514599\n",
      "Iteration 166, loss = 0.18339929\n",
      "Iteration 167, loss = 0.18222830\n",
      "Iteration 168, loss = 0.18049102\n",
      "Iteration 169, loss = 0.17896084\n",
      "Iteration 170, loss = 0.17762853\n",
      "Iteration 171, loss = 0.17612310\n",
      "Iteration 172, loss = 0.17420702\n",
      "Iteration 173, loss = 0.17257612\n",
      "Iteration 174, loss = 0.17106073\n",
      "Iteration 175, loss = 0.16995166\n",
      "Iteration 176, loss = 0.16831025\n",
      "Iteration 177, loss = 0.16695605\n",
      "Iteration 178, loss = 0.16544410\n",
      "Iteration 179, loss = 0.16374200\n",
      "Iteration 180, loss = 0.16263565\n",
      "Iteration 181, loss = 0.16093956\n",
      "Iteration 182, loss = 0.15930322\n",
      "Iteration 183, loss = 0.15802822\n",
      "Iteration 184, loss = 0.15639187\n",
      "Iteration 185, loss = 0.15497697\n",
      "Iteration 186, loss = 0.15338445\n",
      "Iteration 187, loss = 0.15276532\n",
      "Iteration 188, loss = 0.15120784\n",
      "Iteration 189, loss = 0.14923072\n",
      "Iteration 190, loss = 0.14785038\n",
      "Iteration 191, loss = 0.14763734\n",
      "Iteration 192, loss = 0.14570093\n",
      "Iteration 193, loss = 0.14362238\n",
      "Iteration 194, loss = 0.14265353\n",
      "Iteration 195, loss = 0.14093948\n",
      "Iteration 196, loss = 0.14004602\n",
      "Iteration 197, loss = 0.13847665\n",
      "Iteration 198, loss = 0.13801007\n",
      "Iteration 199, loss = 0.13617360\n",
      "Iteration 200, loss = 0.13461536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yardenas/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62083451\n",
      "Iteration 2, loss = 0.57541053\n",
      "Iteration 3, loss = 0.55290951\n",
      "Iteration 4, loss = 0.53698297\n",
      "Iteration 5, loss = 0.52544869\n",
      "Iteration 6, loss = 0.51643265\n",
      "Iteration 7, loss = 0.50947865\n",
      "Iteration 8, loss = 0.50389883\n",
      "Iteration 9, loss = 0.49887346\n",
      "Iteration 10, loss = 0.49458876\n",
      "Iteration 11, loss = 0.49075368\n",
      "Iteration 12, loss = 0.48727309\n",
      "Iteration 13, loss = 0.48389492\n",
      "Iteration 14, loss = 0.48077411\n",
      "Iteration 15, loss = 0.47774922\n",
      "Iteration 16, loss = 0.47482993\n",
      "Iteration 17, loss = 0.47230339\n",
      "Iteration 18, loss = 0.46959080\n",
      "Iteration 19, loss = 0.46690720\n",
      "Iteration 20, loss = 0.46425444\n",
      "Iteration 21, loss = 0.46181398\n",
      "Iteration 22, loss = 0.45942735\n",
      "Iteration 23, loss = 0.45712168\n",
      "Iteration 24, loss = 0.45485880\n",
      "Iteration 25, loss = 0.45260810\n",
      "Iteration 26, loss = 0.45020458\n",
      "Iteration 27, loss = 0.44824090\n",
      "Iteration 28, loss = 0.44571066\n",
      "Iteration 29, loss = 0.44339413\n",
      "Iteration 30, loss = 0.44122742\n",
      "Iteration 31, loss = 0.43904001\n",
      "Iteration 32, loss = 0.43669031\n",
      "Iteration 33, loss = 0.43447342\n",
      "Iteration 34, loss = 0.43250477\n",
      "Iteration 35, loss = 0.43029629\n",
      "Iteration 36, loss = 0.42801242\n",
      "Iteration 37, loss = 0.42607375\n",
      "Iteration 38, loss = 0.42398164\n",
      "Iteration 39, loss = 0.42175887\n",
      "Iteration 40, loss = 0.41958783\n",
      "Iteration 41, loss = 0.41756386\n",
      "Iteration 42, loss = 0.41571317\n",
      "Iteration 43, loss = 0.41345585\n",
      "Iteration 44, loss = 0.41145943\n",
      "Iteration 45, loss = 0.40953825\n",
      "Iteration 46, loss = 0.40727108\n",
      "Iteration 47, loss = 0.40498226\n",
      "Iteration 48, loss = 0.40319822\n",
      "Iteration 49, loss = 0.40078736\n",
      "Iteration 50, loss = 0.39879451\n",
      "Iteration 51, loss = 0.39668006\n",
      "Iteration 52, loss = 0.39482337\n",
      "Iteration 53, loss = 0.39264928\n",
      "Iteration 54, loss = 0.39113668\n",
      "Iteration 55, loss = 0.38852012\n",
      "Iteration 56, loss = 0.38659642\n",
      "Iteration 57, loss = 0.38426606\n",
      "Iteration 58, loss = 0.38252635\n",
      "Iteration 59, loss = 0.38014922\n",
      "Iteration 60, loss = 0.37806284\n",
      "Iteration 61, loss = 0.37590835\n",
      "Iteration 62, loss = 0.37378148\n",
      "Iteration 63, loss = 0.37183100\n",
      "Iteration 64, loss = 0.36960917\n",
      "Iteration 65, loss = 0.36766274\n",
      "Iteration 66, loss = 0.36535518\n",
      "Iteration 67, loss = 0.36344327\n",
      "Iteration 68, loss = 0.36177984\n",
      "Iteration 69, loss = 0.35905546\n",
      "Iteration 70, loss = 0.35718216\n",
      "Iteration 71, loss = 0.35503436\n",
      "Iteration 72, loss = 0.35288664\n",
      "Iteration 73, loss = 0.35077397\n",
      "Iteration 74, loss = 0.34856552\n",
      "Iteration 75, loss = 0.34653920\n",
      "Iteration 76, loss = 0.34451620\n",
      "Iteration 77, loss = 0.34261147\n",
      "Iteration 78, loss = 0.34042523\n",
      "Iteration 79, loss = 0.33846884\n",
      "Iteration 80, loss = 0.33649096\n",
      "Iteration 81, loss = 0.33419417\n",
      "Iteration 82, loss = 0.33205513\n",
      "Iteration 83, loss = 0.32994339\n",
      "Iteration 84, loss = 0.32791380\n",
      "Iteration 85, loss = 0.32589619\n",
      "Iteration 86, loss = 0.32378078\n",
      "Iteration 87, loss = 0.32177650\n",
      "Iteration 88, loss = 0.31969401\n",
      "Iteration 89, loss = 0.31757811\n",
      "Iteration 90, loss = 0.31521941\n",
      "Iteration 91, loss = 0.31330570\n",
      "Iteration 92, loss = 0.31094353\n",
      "Iteration 93, loss = 0.30933750\n",
      "Iteration 94, loss = 0.30754319\n",
      "Iteration 95, loss = 0.30464138\n",
      "Iteration 96, loss = 0.30339498\n",
      "Iteration 97, loss = 0.30135414\n",
      "Iteration 98, loss = 0.29905441\n",
      "Iteration 99, loss = 0.29653492\n",
      "Iteration 100, loss = 0.29480256\n",
      "Iteration 101, loss = 0.29247835\n",
      "Iteration 102, loss = 0.29088701\n",
      "Iteration 103, loss = 0.28850496\n",
      "Iteration 104, loss = 0.28655361\n",
      "Iteration 105, loss = 0.28469213\n",
      "Iteration 106, loss = 0.28290873\n",
      "Iteration 107, loss = 0.28109302\n",
      "Iteration 108, loss = 0.27914466\n",
      "Iteration 109, loss = 0.27690223\n",
      "Iteration 110, loss = 0.27579660\n",
      "Iteration 111, loss = 0.27323188\n",
      "Iteration 112, loss = 0.27099331\n",
      "Iteration 113, loss = 0.26928438\n",
      "Iteration 114, loss = 0.26742895\n",
      "Iteration 115, loss = 0.26528723\n",
      "Iteration 116, loss = 0.26311474\n",
      "Iteration 117, loss = 0.26156186\n",
      "Iteration 118, loss = 0.25948276\n",
      "Iteration 119, loss = 0.25762946\n",
      "Iteration 120, loss = 0.25575548\n",
      "Iteration 121, loss = 0.25414754\n",
      "Iteration 122, loss = 0.25230539\n",
      "Iteration 123, loss = 0.25048789\n",
      "Iteration 124, loss = 0.24839716\n",
      "Iteration 125, loss = 0.24613780\n",
      "Iteration 126, loss = 0.24459501\n",
      "Iteration 127, loss = 0.24325790\n",
      "Iteration 128, loss = 0.24151922\n",
      "Iteration 129, loss = 0.23939821\n",
      "Iteration 130, loss = 0.23737971\n",
      "Iteration 131, loss = 0.23533315\n",
      "Iteration 132, loss = 0.23401656\n",
      "Iteration 133, loss = 0.23185824\n",
      "Iteration 134, loss = 0.22978971\n",
      "Iteration 135, loss = 0.22880373\n",
      "Iteration 136, loss = 0.22638735\n",
      "Iteration 137, loss = 0.22526878\n",
      "Iteration 138, loss = 0.22330218\n",
      "Iteration 139, loss = 0.22159200\n",
      "Iteration 140, loss = 0.21926401\n",
      "Iteration 141, loss = 0.21764041\n",
      "Iteration 142, loss = 0.21631171\n",
      "Iteration 143, loss = 0.21453495\n",
      "Iteration 144, loss = 0.21318568\n",
      "Iteration 145, loss = 0.21139368\n",
      "Iteration 146, loss = 0.20983817\n",
      "Iteration 147, loss = 0.20769640\n",
      "Iteration 148, loss = 0.20654393\n",
      "Iteration 149, loss = 0.20449538\n",
      "Iteration 150, loss = 0.20278907\n",
      "Iteration 151, loss = 0.20116586\n",
      "Iteration 152, loss = 0.19925622\n",
      "Iteration 153, loss = 0.19787813\n",
      "Iteration 154, loss = 0.19643968\n",
      "Iteration 155, loss = 0.19497578\n",
      "Iteration 156, loss = 0.19307178\n",
      "Iteration 157, loss = 0.19208355\n",
      "Iteration 158, loss = 0.18997245\n",
      "Iteration 159, loss = 0.18815768\n",
      "Iteration 160, loss = 0.18661494\n",
      "Iteration 161, loss = 0.18533433\n",
      "Iteration 162, loss = 0.18353010\n",
      "Iteration 163, loss = 0.18152330\n",
      "Iteration 164, loss = 0.17990668\n",
      "Iteration 165, loss = 0.17876780\n",
      "Iteration 166, loss = 0.17708967\n",
      "Iteration 167, loss = 0.17577345\n",
      "Iteration 168, loss = 0.17396105\n",
      "Iteration 169, loss = 0.17207384\n",
      "Iteration 170, loss = 0.17135621\n",
      "Iteration 171, loss = 0.16938872\n",
      "Iteration 172, loss = 0.16755939\n",
      "Iteration 173, loss = 0.16624412\n",
      "Iteration 174, loss = 0.16468233\n",
      "Iteration 175, loss = 0.16354989\n",
      "Iteration 176, loss = 0.16189683\n",
      "Iteration 177, loss = 0.16004742\n",
      "Iteration 178, loss = 0.15873752\n",
      "Iteration 179, loss = 0.15734906\n",
      "Iteration 180, loss = 0.15585328\n",
      "Iteration 181, loss = 0.15440374\n",
      "Iteration 182, loss = 0.15331386\n",
      "Iteration 183, loss = 0.15145261\n",
      "Iteration 184, loss = 0.14998835\n",
      "Iteration 185, loss = 0.14858738\n",
      "Iteration 186, loss = 0.14705982\n",
      "Iteration 187, loss = 0.14574799\n",
      "Iteration 188, loss = 0.14404710\n",
      "Iteration 189, loss = 0.14289057\n",
      "Iteration 190, loss = 0.14183033\n",
      "Iteration 191, loss = 0.13967956\n",
      "Iteration 192, loss = 0.13869442\n",
      "Iteration 193, loss = 0.13681322\n",
      "Iteration 194, loss = 0.13579927\n",
      "Iteration 195, loss = 0.13422614\n",
      "Iteration 196, loss = 0.13290236\n",
      "Iteration 197, loss = 0.13173523\n",
      "Iteration 198, loss = 0.13018416\n",
      "Iteration 199, loss = 0.12876723\n",
      "Iteration 200, loss = 0.12739298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yardenas/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.59920141\n",
      "Iteration 2, loss = 0.56833947\n",
      "Iteration 3, loss = 0.54932117\n",
      "Iteration 4, loss = 0.53528107\n",
      "Iteration 5, loss = 0.52456178\n",
      "Iteration 6, loss = 0.51619126\n",
      "Iteration 7, loss = 0.50964691\n",
      "Iteration 8, loss = 0.50391122\n",
      "Iteration 9, loss = 0.49918485\n",
      "Iteration 10, loss = 0.49516690\n",
      "Iteration 11, loss = 0.49153370\n",
      "Iteration 12, loss = 0.48818221\n",
      "Iteration 13, loss = 0.48474202\n",
      "Iteration 14, loss = 0.48198675\n",
      "Iteration 15, loss = 0.47901564\n",
      "Iteration 16, loss = 0.47614331\n",
      "Iteration 17, loss = 0.47357077\n",
      "Iteration 18, loss = 0.47107074\n",
      "Iteration 19, loss = 0.46864699\n",
      "Iteration 20, loss = 0.46606604\n",
      "Iteration 21, loss = 0.46375423\n",
      "Iteration 22, loss = 0.46158594\n",
      "Iteration 23, loss = 0.45920422\n",
      "Iteration 24, loss = 0.45691454\n",
      "Iteration 25, loss = 0.45464791\n",
      "Iteration 26, loss = 0.45239359\n",
      "Iteration 27, loss = 0.45021661\n",
      "Iteration 28, loss = 0.44816856\n",
      "Iteration 29, loss = 0.44614757\n",
      "Iteration 30, loss = 0.44403618\n",
      "Iteration 31, loss = 0.44189422\n",
      "Iteration 32, loss = 0.43989000\n",
      "Iteration 33, loss = 0.43792943\n",
      "Iteration 34, loss = 0.43573759\n",
      "Iteration 35, loss = 0.43367200\n",
      "Iteration 36, loss = 0.43147408\n",
      "Iteration 37, loss = 0.42945726\n",
      "Iteration 38, loss = 0.42739486\n",
      "Iteration 39, loss = 0.42535790\n",
      "Iteration 40, loss = 0.42326751\n",
      "Iteration 41, loss = 0.42128295\n",
      "Iteration 42, loss = 0.41880838\n",
      "Iteration 43, loss = 0.41678942\n",
      "Iteration 44, loss = 0.41461094\n",
      "Iteration 45, loss = 0.41261972\n",
      "Iteration 46, loss = 0.41059441\n",
      "Iteration 47, loss = 0.40839599\n",
      "Iteration 48, loss = 0.40647072\n",
      "Iteration 49, loss = 0.40412702\n",
      "Iteration 50, loss = 0.40202579\n",
      "Iteration 51, loss = 0.40012202\n",
      "Iteration 52, loss = 0.39786286\n",
      "Iteration 53, loss = 0.39580874\n",
      "Iteration 54, loss = 0.39327461\n",
      "Iteration 55, loss = 0.39119423\n",
      "Iteration 56, loss = 0.38926789\n",
      "Iteration 57, loss = 0.38715335\n",
      "Iteration 58, loss = 0.38504989\n",
      "Iteration 59, loss = 0.38314489\n",
      "Iteration 60, loss = 0.38079524\n",
      "Iteration 61, loss = 0.37891285\n",
      "Iteration 62, loss = 0.37663875\n",
      "Iteration 63, loss = 0.37438899\n",
      "Iteration 64, loss = 0.37223744\n",
      "Iteration 65, loss = 0.37036571\n",
      "Iteration 66, loss = 0.36795499\n",
      "Iteration 67, loss = 0.36587127\n",
      "Iteration 68, loss = 0.36397766\n",
      "Iteration 69, loss = 0.36176607\n",
      "Iteration 70, loss = 0.36008998\n",
      "Iteration 71, loss = 0.35749482\n",
      "Iteration 72, loss = 0.35551971\n",
      "Iteration 73, loss = 0.35352732\n",
      "Iteration 74, loss = 0.35163426\n",
      "Iteration 75, loss = 0.34961042\n",
      "Iteration 76, loss = 0.34752196\n",
      "Iteration 77, loss = 0.34524733\n",
      "Iteration 78, loss = 0.34294459\n",
      "Iteration 79, loss = 0.34147337\n",
      "Iteration 80, loss = 0.33879424\n",
      "Iteration 81, loss = 0.33689174\n",
      "Iteration 82, loss = 0.33457340\n",
      "Iteration 83, loss = 0.33262986\n",
      "Iteration 84, loss = 0.33047910\n",
      "Iteration 85, loss = 0.32854104\n",
      "Iteration 86, loss = 0.32640216\n",
      "Iteration 87, loss = 0.32448219\n",
      "Iteration 88, loss = 0.32238380\n",
      "Iteration 89, loss = 0.32032588\n",
      "Iteration 90, loss = 0.31827441\n",
      "Iteration 91, loss = 0.31617619\n",
      "Iteration 92, loss = 0.31464216\n",
      "Iteration 93, loss = 0.31252003\n",
      "Iteration 94, loss = 0.30996757\n",
      "Iteration 95, loss = 0.30808564\n",
      "Iteration 96, loss = 0.30688534\n",
      "Iteration 97, loss = 0.30407144\n",
      "Iteration 98, loss = 0.30201187\n",
      "Iteration 99, loss = 0.30040001\n",
      "Iteration 100, loss = 0.29884917\n",
      "Iteration 101, loss = 0.29648697\n",
      "Iteration 102, loss = 0.29451810\n",
      "Iteration 103, loss = 0.29232427\n",
      "Iteration 104, loss = 0.29061967\n",
      "Iteration 105, loss = 0.28831583\n",
      "Iteration 106, loss = 0.28654401\n",
      "Iteration 107, loss = 0.28425548\n",
      "Iteration 108, loss = 0.28259909\n",
      "Iteration 109, loss = 0.28046011\n",
      "Iteration 110, loss = 0.27918471\n",
      "Iteration 111, loss = 0.27666613\n",
      "Iteration 112, loss = 0.27465822\n",
      "Iteration 113, loss = 0.27247231\n",
      "Iteration 114, loss = 0.27054316\n",
      "Iteration 115, loss = 0.26893816\n",
      "Iteration 116, loss = 0.26736720\n",
      "Iteration 117, loss = 0.26516432\n",
      "Iteration 118, loss = 0.26328167\n",
      "Iteration 119, loss = 0.26151445\n",
      "Iteration 120, loss = 0.26015020\n",
      "Iteration 121, loss = 0.25766620\n",
      "Iteration 122, loss = 0.25565257\n",
      "Iteration 123, loss = 0.25415018\n",
      "Iteration 124, loss = 0.25217777\n",
      "Iteration 125, loss = 0.25084550\n",
      "Iteration 126, loss = 0.24829858\n",
      "Iteration 127, loss = 0.24747577\n",
      "Iteration 128, loss = 0.24517758\n",
      "Iteration 129, loss = 0.24343342\n",
      "Iteration 130, loss = 0.24184571\n",
      "Iteration 131, loss = 0.24046777\n",
      "Iteration 132, loss = 0.23830690\n",
      "Iteration 133, loss = 0.23695346\n",
      "Iteration 134, loss = 0.23504520\n",
      "Iteration 135, loss = 0.23344385\n",
      "Iteration 136, loss = 0.23139158\n",
      "Iteration 137, loss = 0.23024242\n",
      "Iteration 138, loss = 0.22802792\n",
      "Iteration 139, loss = 0.22657185\n",
      "Iteration 140, loss = 0.22458686\n",
      "Iteration 141, loss = 0.22367750\n",
      "Iteration 142, loss = 0.22196200\n",
      "Iteration 143, loss = 0.21988277\n",
      "Iteration 144, loss = 0.21895454\n",
      "Iteration 145, loss = 0.21634375\n",
      "Iteration 146, loss = 0.21455400\n",
      "Iteration 147, loss = 0.21323973\n",
      "Iteration 148, loss = 0.21191105\n",
      "Iteration 149, loss = 0.21030556\n",
      "Iteration 150, loss = 0.20867973\n",
      "Iteration 151, loss = 0.20722539\n",
      "Iteration 152, loss = 0.20513426\n",
      "Iteration 153, loss = 0.20331905\n",
      "Iteration 154, loss = 0.20202416\n",
      "Iteration 155, loss = 0.20034155\n",
      "Iteration 156, loss = 0.19852412\n",
      "Iteration 157, loss = 0.19717835\n",
      "Iteration 158, loss = 0.19572106\n",
      "Iteration 159, loss = 0.19452204\n",
      "Iteration 160, loss = 0.19258078\n",
      "Iteration 161, loss = 0.19148757\n",
      "Iteration 162, loss = 0.18946232\n",
      "Iteration 163, loss = 0.18788543\n",
      "Iteration 164, loss = 0.18629261\n",
      "Iteration 165, loss = 0.18443432\n",
      "Iteration 166, loss = 0.18326883\n",
      "Iteration 167, loss = 0.18223753\n",
      "Iteration 168, loss = 0.18057071\n",
      "Iteration 169, loss = 0.17873726\n",
      "Iteration 170, loss = 0.17707694\n",
      "Iteration 171, loss = 0.17546627\n",
      "Iteration 172, loss = 0.17410805\n",
      "Iteration 173, loss = 0.17254056\n",
      "Iteration 174, loss = 0.17116282\n",
      "Iteration 175, loss = 0.16986053\n",
      "Iteration 176, loss = 0.16853093\n",
      "Iteration 177, loss = 0.16641043\n",
      "Iteration 178, loss = 0.16504681\n",
      "Iteration 179, loss = 0.16379474\n",
      "Iteration 180, loss = 0.16252782\n",
      "Iteration 181, loss = 0.16107366\n",
      "Iteration 182, loss = 0.15975621\n",
      "Iteration 183, loss = 0.15822130\n",
      "Iteration 184, loss = 0.15692778\n",
      "Iteration 185, loss = 0.15496828\n",
      "Iteration 186, loss = 0.15435060\n",
      "Iteration 187, loss = 0.15251189\n",
      "Iteration 188, loss = 0.15095138\n",
      "Iteration 189, loss = 0.14974327\n",
      "Iteration 190, loss = 0.14817314\n",
      "Iteration 191, loss = 0.14753794\n",
      "Iteration 192, loss = 0.14592782\n",
      "Iteration 193, loss = 0.14428740\n",
      "Iteration 194, loss = 0.14277639\n",
      "Iteration 195, loss = 0.14153646\n",
      "Iteration 196, loss = 0.14047620\n",
      "Iteration 197, loss = 0.13866542\n",
      "Iteration 198, loss = 0.13784354\n",
      "Iteration 199, loss = 0.13615170\n",
      "Iteration 200, loss = 0.13501775\n",
      "Iteration 1, loss = 0.62308230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yardenas/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.53662555\n",
      "Iteration 3, loss = 0.49572966\n",
      "Iteration 4, loss = 0.46753403\n",
      "Iteration 5, loss = 0.44848358\n",
      "Iteration 6, loss = 0.43605188\n",
      "Iteration 7, loss = 0.42747243\n",
      "Iteration 8, loss = 0.42085867\n",
      "Iteration 9, loss = 0.41545721\n",
      "Iteration 10, loss = 0.41091615\n",
      "Iteration 11, loss = 0.40712039\n",
      "Iteration 12, loss = 0.40327820\n",
      "Iteration 13, loss = 0.40013775\n",
      "Iteration 14, loss = 0.39714277\n",
      "Iteration 15, loss = 0.39400912\n",
      "Iteration 16, loss = 0.39124236\n",
      "Iteration 17, loss = 0.38874194\n",
      "Iteration 18, loss = 0.38616358\n",
      "Iteration 19, loss = 0.38411207\n",
      "Iteration 20, loss = 0.38168552\n",
      "Iteration 21, loss = 0.37945697\n",
      "Iteration 22, loss = 0.37711695\n",
      "Iteration 23, loss = 0.37507691\n",
      "Iteration 24, loss = 0.37313858\n",
      "Iteration 25, loss = 0.37101169\n",
      "Iteration 26, loss = 0.36898047\n",
      "Iteration 27, loss = 0.36722016\n",
      "Iteration 28, loss = 0.36511379\n",
      "Iteration 29, loss = 0.36360639\n",
      "Iteration 30, loss = 0.36173428\n",
      "Iteration 31, loss = 0.35943351\n",
      "Iteration 32, loss = 0.35756452\n",
      "Iteration 33, loss = 0.35568584\n",
      "Iteration 34, loss = 0.35384052\n",
      "Iteration 35, loss = 0.35187999\n",
      "Iteration 36, loss = 0.35001892\n",
      "Iteration 37, loss = 0.34811886\n",
      "Iteration 38, loss = 0.34624580\n",
      "Iteration 39, loss = 0.34423172\n",
      "Iteration 40, loss = 0.34243443\n",
      "Iteration 41, loss = 0.34053618\n",
      "Iteration 42, loss = 0.33851360\n",
      "Iteration 43, loss = 0.33663684\n",
      "Iteration 44, loss = 0.33464860\n",
      "Iteration 45, loss = 0.33288662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import (GridSearchCV,\n",
    "    cross_val_score, KFold)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "                    SimpleImputer(strategy='median'),\n",
    "                    StandardScaler(),\n",
    "                    OneVsRestClassifier(\n",
    "                        MLPClassifier(\n",
    "                        hidden_layer_sizes=(64, 64, 64),\n",
    "                        verbose=True)))\n",
    "parameter_space = {\n",
    "    'onevsrestclassifier__estimator__learning_rate_init':\n",
    "    np.logspace(-4, -2, 1)\n",
    "}\n",
    "inner_cv = KFold(n_splits=2, shuffle=True)\n",
    "outer_cv = KFold(n_splits=2, shuffle=True)\n",
    "classifier = GridSearchCV(pipeline, parameter_space,\n",
    "                               n_jobs=-1, scoring='roc_auc',\n",
    "                               iid=True,\n",
    "                               refit=True,\n",
    "                               cv=inner_cv,\n",
    "                               verbose=True)\n",
    "scores = cross_val_score(classifier, x_train, y_train,\n",
    "                            cv=outer_cv,\n",
    "                            scoring='roc_auc')\n",
    "print(\"Cross-validation score is {score:.3f},\"\n",
    "      \" standard deviation is {err:.3f}\"\n",
    "      .format(score = scores.mean(), err = scores.std()))\n",
    "classifier = classifier.fit(x_train, y_train.ravel())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
