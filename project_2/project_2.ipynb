{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro ML - Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_features.csv')\n",
    "labels = pd.read_csv('train_labels.csv')\n",
    "test_data = pd.read_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time_features(data, n_samples):\n",
    "    x = []\n",
    "    features = [np.nanmedian, np.nanmean, np.nanvar, np.nanmin,\n",
    "           np.nanmax]\n",
    "    for index in range(int(data.shape[0] / n_samples)):\n",
    "        assert data[n_samples * index, 0] == data[n_samples * (index + 1) - 1, 0], \\\n",
    "        'Ids are {}, {}'.format(data[n_samples * index, 0], data[n_samples * (index + 1) - 1, 0])\n",
    "        patient_data = data[n_samples * index:n_samples * (index + 1), 2:]\n",
    "        feature_values = np.empty((len(features), data[:, 2:].shape[1]))\n",
    "        for i, feature in enumerate(features):\n",
    "            feature_values[i] = feature(patient_data, axis=0)\n",
    "        x.append(feature_values.ravel())\n",
    "    return np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yardenas/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py:3405: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n",
      "/home/yardenas/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: Mean of empty slice\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/yardenas/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/home/yardenas/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: All-NaN slice encountered\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "x_train = calculate_time_features(train_data.to_numpy(), 12)\n",
    "x_test = calculate_time_features(test_data.to_numpy(), 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Pipeline - Subtask 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtask1_labels_ids = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST',\n",
    "         'LABEL_Alkalinephos', 'LABEL_Bilirubin_total', \n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "y_train = labels[subtask1_labels_ids].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import (GridSearchCV,\n",
    "    cross_val_score, KFold)\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# TODO (yarden):\n",
    "# feature selection.\n",
    "# parameters tuning (subsample, learning rate).\n",
    "pipeline = make_pipeline(\n",
    "                    SimpleImputer(strategy='median'),\n",
    "                    StandardScaler(),\n",
    "                    OneVsRestClassifier(\n",
    "                        GradientBoostingClassifier(subsample=0.5)))\n",
    "scores = cross_val_score(pipeline, x_train, y_train,\n",
    "                            cv=5,\n",
    "                            scoring='roc_auc',\n",
    "                            verbose=True)\n",
    "print(\"Cross-validation score is {score:.3f},\"\n",
    "      \" standard deviation is {err:.3f}\"\n",
    "      .format(score = scores.mean(), err = scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed: 19.8min finished\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline.fit(x_train, y_train)\n",
    "predictions = pipeline.predict_proba(x_test)\n",
    "df = pd.DataFrame({'pid': test_data.iloc[0::12, 0]})\n",
    "print(\"Training score:\", metrics.roc_auc_score(y_train, classifier.predict_proba(x_train)))\n",
    "for i, label in enumerate(subtask1_labels_ids):\n",
    "    df[label] = predictions[:, i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Pipeline - Subtask 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtask2_labels_ids = ['LABEL_Sepsis']\n",
    "y_train = labels[subtask2_labels_ids].to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import (GridSearchCV,\n",
    "    cross_val_score, KFold)\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# TODO (yarden):\n",
    "# feature selection.\n",
    "# parameters tuning (subsample, learning rate).\n",
    "pipeline = make_pipeline(\n",
    "                    SimpleImputer(strategy='median'),\n",
    "                    StandardScaler(),\n",
    "                    GradientBoostingClassifier(subsample=0.5))\n",
    "\n",
    "scores = cross_val_score(pipeline, x_train, y_train,\n",
    "                            cv=5,\n",
    "                            scoring='roc_auc',\n",
    "                            verbose=True)\n",
    "print(\"Cross-validation score is {score:.3f},\"\n",
    "      \" standard deviation is {err:.3f}\"\n",
    "      .format(score = scores.mean(), err = scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.8053847041413316\n"
     ]
    }
   ],
   "source": [
    "pipeline = pipeline.fit(x_train, y_train)\n",
    "predictions = pipeline.predict_proba(x_test)[:, 1]\n",
    "print(\"Training score:\", metrics.roc_auc_score(y_train, classifier.predict_proba(x_train)[:, 1]))\n",
    "df[subtask2_labels_ids] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Pipeline - Subtask 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtask3_labels_ids = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2',\n",
    "                      'LABEL_Heartrate']\n",
    "y_train = labels[subtask3_labels_ids].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import (GridSearchCV,\n",
    "    cross_val_score, KFold)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "for i, label in enumerate(subtask3_labels_ids):\n",
    "pipeline = make_pipeline(\n",
    "                    SimpleImputer(strategy='median'),\n",
    "                    StandardScaler(),\n",
    "                    GradientBoostingRegressor())\n",
    "\n",
    "classifier = GridSearchCV(pipeline, parameter_space,\n",
    "                               n_jobs=-1, scoring='r2_score',\n",
    "                               iid=True,\n",
    "                               refit=True,\n",
    "                               cv=inner_cv,\n",
    "                               verbose=True)\n",
    "scores = cross_val_score(classifier, x_train, y_train,\n",
    "                            cv=outer_cv,\n",
    "                            scoring='r2_score',\n",
    "                            verbose=True)\n",
    "print(\"Cross-validation score is {score:.3f},\"\n",
    "      \" standard deviation is {err:.3f}\"\n",
    "      .format(score = scores.mean(), err = scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score: 0.8656996972763592\n",
      "Training score: 0.8222861653906491\n",
      "Training score: 0.5295847293484595\n",
      "Training score: 0.8691216850675991\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "for i, label in enumerate(subtask3_labels_ids):\n",
    "    regressor = MLPRegressor(hidden_layer_sizes=(64, 64, 64),\n",
    "                            learning_rate_init=0.003,\n",
    "                             max_iter=300) if i != 2 else \\\n",
    "    GradientBoostingRegressor(subsample=0.5)\n",
    "    pipeline = make_pipeline(\n",
    "                        SimpleImputer(strategy='median'),\n",
    "                        StandardScaler(),\n",
    "                        regressor)\n",
    "    pipeline = pipeline.fit(x_train, y_train[:, i])\n",
    "    predictions = pipeline.predict(x_test)\n",
    "    print(\"Training score:\", metrics.r2_score(y_train[:, i], pipeline.predict(x_train)))\n",
    "    df[label] = predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('prediction.zip', index=False, float_format='%.4f', compression='zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
